import pandas as pd
################! comment lines generated by ChatGPT 3.5
#It imports the pandas library as pd, which is used for working with data in tabular form (DataFrames).
#parameters: csvIn, csvOut, c1, and c2.
#csvIn: Represents the input CSV file path.
#csvOut: Represents the output CSV file path.
#c1: Represents the name of the first field (key 1) based on which duplicates will be removed.
#c2: Represents the name of the second field (key 2) based on which duplicates will be removed.
#It reads the input CSV file csvIn into a pandas DataFrame df.
#It sorts the DataFrame df based on the columns c1 and c2 in ascending order.
#It removes duplicates from the sorted DataFrame, 
#   retaining the last occurrence of each duplicate based on columns c1 and c2.
#It writes the resulting DataFrame to the output CSV file csvOut.
######
#The script then iterates through rows of another CSV file uFile using pd.read_csv(uFile) 
# to read the file into a DataFrame csvFileList. 
# This DataFrame appears to contain information about input and output CSV files 
# and the two field names (Key1 and Key2).
# Inside the loop, it extracts information from each row of the csvFileList DataFrame, 
# including csvIn, csvOut, c1, and c2.
# It calls the cleanDuplicateRowsFromTop function with these parameters to clean duplicates 
# in each input CSV file and save the cleaned data to the corresponding output CSV file.
# It prints a message indicating that the cleaning process is completed after processing 
# all rows in the csvFileList DataFrame.
#######
#Overall, this script is designed to clean duplicate rows from multiple CSV files, 
#where the columns based on which duplicates are removed are specified in 
#the csvFileList DataFrame for each file. The cleaned data is saved in separate output CSV files.
################!
#Define cleanning function.
def cleanDuplicateRowsFromTop(csvIn,csvOut,c1,c2):
    # Read the CSV file into a DataFrame
    data = pd.read_csv(csvIn)
    # Create a DataFrame
    df = pd.DataFrame(data)
    # Remove duplicates based on two fields (Field1 and Field2)
    df_sorted = df.sort_values(by=[c1,c2], ascending=[True, True])
    df_no_duplicates = df_sorted.drop_duplicates(subset=[c1, c2],keep='last')
    # Write the DataFrame to a CSV file
    # Set index=False to exclude row numbers in the CSV file
    df_no_duplicates.to_csv(csvOut, index=False)  

# Iterate through rows
# Read the CSV file into a DataFrame
uFile=r'mergedFileList_forDuplicateRemoval.csv' #replace your file with list of the four fields as requried in the loop below.
csvFileList = pd.read_csv(uFile)
uCSVinfo = pd.DataFrame(csvFileList)
#print(csvFileList)
for index, row in uCSVinfo.iterrows():
    csvIn =row["SourceFile"]
    csvOut =row["DestinationFile"]
    c1=row["FieldName(Key1)"]
    c2=row["FieldName(Key2)"]
    print("Cleaning file:" + csvIn)
    cleanDuplicateRowsFromTop(csvIn,csvOut,c1,c2)
print("Cleaning process completed....")